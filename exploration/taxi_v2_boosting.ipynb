{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_feather, read_csv\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lat1, lng1, lat2, lng2):\n",
    "    #return distance as meter if you want km distance, remove \"* 1000\"\n",
    "    radius = 6371\n",
    "\n",
    "    dLat = (lat2-lat1) * np.pi / 180\n",
    "    dLng = (lng2-lng1) * np.pi / 180\n",
    "\n",
    "    lat1 = lat1 * np.pi / 180\n",
    "    lat2 = lat2 * np.pi / 180\n",
    "\n",
    "    val = np.sin(dLat/2) * np.sin(dLat/2) + np.sin(dLng/2)\\\n",
    "    * np.sin(dLng/2) * np.cos(lat1) * np.cos(lat2)    \n",
    "    ang = 2 * np.arctan2(np.sqrt(val), np.sqrt(1-val))\n",
    "    return radius * ang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9914, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-27 13:08:24.0000002</td>\n",
       "      <td>2015-01-27 13:08:24</td>\n",
       "      <td>-73.973320</td>\n",
       "      <td>40.763805</td>\n",
       "      <td>-73.981430</td>\n",
       "      <td>40.743835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-27 13:08:24.0000003</td>\n",
       "      <td>2015-01-27 13:08:24</td>\n",
       "      <td>-73.986862</td>\n",
       "      <td>40.719383</td>\n",
       "      <td>-73.998886</td>\n",
       "      <td>40.739201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-08 11:53:44.0000002</td>\n",
       "      <td>2011-10-08 11:53:44</td>\n",
       "      <td>-73.982524</td>\n",
       "      <td>40.751260</td>\n",
       "      <td>-73.979654</td>\n",
       "      <td>40.746139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-01 21:12:12.0000002</td>\n",
       "      <td>2012-12-01 21:12:12</td>\n",
       "      <td>-73.981160</td>\n",
       "      <td>40.767807</td>\n",
       "      <td>-73.990448</td>\n",
       "      <td>40.751635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-01 21:12:12.0000003</td>\n",
       "      <td>2012-12-01 21:12:12</td>\n",
       "      <td>-73.966046</td>\n",
       "      <td>40.789775</td>\n",
       "      <td>-73.988565</td>\n",
       "      <td>40.744427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key     pickup_datetime  pickup_longitude  \\\n",
       "0  2015-01-27 13:08:24.0000002 2015-01-27 13:08:24        -73.973320   \n",
       "1  2015-01-27 13:08:24.0000003 2015-01-27 13:08:24        -73.986862   \n",
       "2  2011-10-08 11:53:44.0000002 2011-10-08 11:53:44        -73.982524   \n",
       "3  2012-12-01 21:12:12.0000002 2012-12-01 21:12:12        -73.981160   \n",
       "4  2012-12-01 21:12:12.0000003 2012-12-01 21:12:12        -73.966046   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0        40.763805         -73.981430         40.743835                1  \n",
       "1        40.719383         -73.998886         40.739201                1  \n",
       "2        40.751260         -73.979654         40.746139                1  \n",
       "3        40.767807         -73.990448         40.751635                1  \n",
       "4        40.789775         -73.988565         40.744427                1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No missing values\n",
    "test_df = read_csv('test.csv', parse_dates=[\"pickup_datetime\"],\n",
    "                  infer_datetime_format=True)\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lng1min = test_df.pickup_longitude.min()\n",
    "lng2min = test_df.dropoff_longitude.min()\n",
    "lat1min = test_df.pickup_latitude.min()\n",
    "lat2min = test_df.dropoff_latitude.min()\n",
    "#\n",
    "lng1max = test_df.pickup_longitude.max()\n",
    "lng2max = test_df.dropoff_longitude.max()\n",
    "lat1max = test_df.pickup_latitude.max()\n",
    "lat2max = test_df.dropoff_latitude.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66.5 ms, sys: 97.9 ms, total: 164 ms\n",
      "Wall time: 680 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read a subsample (train1) of the original dataset with 55M rows\n",
    "# It was obtained ramdomly using the command line subsample task:\n",
    "# >subsample --reservoir -n 1000000 train.csv -r > train1.csv\n",
    "train_df = read_feather('tmp/train1.feather')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000000 entries, 0 to 1999999\n",
      "Data columns (total 7 columns):\n",
      "fare_amount          float32\n",
      "pickup_datetime      datetime64[ns]\n",
      "pickup_longitude     float32\n",
      "pickup_latitude      float32\n",
      "dropoff_longitude    float32\n",
      "dropoff_latitude     float32\n",
      "passenger_count      uint8\n",
      "dtypes: datetime64[ns](1), float32(5), uint8(1)\n",
      "memory usage: 55.3 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(train_df):\n",
    "    \n",
    "    # Remove missing values\n",
    "    train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "\n",
    "    # Remove absurd passenger_count\n",
    "    train_df = train_df[(train_df['passenger_count'] >= 1) &\n",
    "                    (train_df['passenger_count'] <= 6)]\n",
    "\n",
    "    # Remove negative and extreme fare_amount values\n",
    "    train_df = train_df[(train_df['fare_amount'] >= 2.5) & (train_df['fare_amount'] <= 300)]\n",
    "    print('Maximum fare_amount: %.1f' % train_df['fare_amount'].max())\n",
    "\n",
    "    # Remove no displacements\n",
    "    train_df = train_df[(train_df['pickup_latitude'] != train_df['dropoff_latitude'])]\n",
    "    train_df = train_df[(train_df['pickup_longitude'] != train_df['dropoff_longitude'])]\n",
    "\n",
    "    # Remove absurd displacements\n",
    "    train_df = train_df[(train_df['pickup_longitude'] >= lng1min) & (train_df['pickup_longitude'] <= lng1max)]\n",
    "    train_df = train_df[(train_df['dropoff_longitude'] >= lng2min) & (train_df['dropoff_longitude'] <= lng2max)]\n",
    "    train_df = train_df[(train_df['pickup_latitude'] >= lat1min) & (train_df['pickup_latitude'] <= lat1max)]\n",
    "    train_df = train_df[(train_df['dropoff_latitude'] >= lat2min) & (train_df['dropoff_latitude'] <= lat2max)]\n",
    "\n",
    "    # Create new features - distance\n",
    "    train_df['dist'] = distance(train_df['pickup_latitude'], train_df['pickup_longitude'],\n",
    "                                train_df['dropoff_latitude'], train_df['dropoff_longitude'])\n",
    "    #train_df = train_df[train_df['dist'] < 100.01]\n",
    "\n",
    "    # Create new features - dayofweek,hour,month,year\n",
    "    train_df['dayofweek'] = train_df['pickup_datetime'].dt.dayofweek.astype('uint8')\n",
    "    train_df['hour'] = train_df['pickup_datetime'].dt.hour.astype('uint8')\n",
    "    train_df['month'] = train_df['pickup_datetime'].dt.month.astype('uint8')\n",
    "    train_df['year'] = train_df['pickup_datetime'].dt.year.astype('uint16')\n",
    "\n",
    "    # Create dataframes for the two periods\n",
    "    P1 = train_df[(train_df['pickup_datetime'] < '2012-09-01')]\n",
    "    P1 = P1.drop(['pickup_datetime'], axis=1)\n",
    "    print(P1.shape)\n",
    "    P2 = train_df.loc[(train_df['pickup_datetime'] >= '2012-09-01')]\n",
    "    P2 = P2.drop(['pickup_datetime'], axis=1)\n",
    "    print(P2.shape)\n",
    "\n",
    "    # Save memory\n",
    "    print(train_df.shape)\n",
    "    print('Maximum ride distance: %.1f' % train_df['dist'].max())\n",
    "    del train_df\n",
    "    return P1,P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum fare_amount: 300.0\n",
      "(1099637, 11)\n",
      "(827729, 11)\n",
      "(1927366, 12)\n",
      "Maximum ride distance: 115.2\n"
     ]
    }
   ],
   "source": [
    "P1, P2 = transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>dist</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.669998</td>\n",
       "      <td>-73.870819</td>\n",
       "      <td>40.773991</td>\n",
       "      <td>-73.999054</td>\n",
       "      <td>40.760658</td>\n",
       "      <td>1</td>\n",
       "      <td>10.900634</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.300000</td>\n",
       "      <td>-73.980865</td>\n",
       "      <td>40.750500</td>\n",
       "      <td>-73.981079</td>\n",
       "      <td>40.755962</td>\n",
       "      <td>1</td>\n",
       "      <td>0.607685</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>-73.979965</td>\n",
       "      <td>40.743340</td>\n",
       "      <td>-73.988792</td>\n",
       "      <td>40.759567</td>\n",
       "      <td>1</td>\n",
       "      <td>1.951639</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.100000</td>\n",
       "      <td>-74.002220</td>\n",
       "      <td>40.738979</td>\n",
       "      <td>-74.003189</td>\n",
       "      <td>40.732300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.747204</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.100000</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.772606</td>\n",
       "      <td>-73.968719</td>\n",
       "      <td>40.751492</td>\n",
       "      <td>1</td>\n",
       "      <td>2.606266</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0     34.669998        -73.870819        40.773991         -73.999054   \n",
       "2      3.300000        -73.980865        40.750500         -73.981079   \n",
       "3      7.300000        -73.979965        40.743340         -73.988792   \n",
       "8      4.100000        -74.002220        40.738979         -74.003189   \n",
       "10    12.100000        -73.982155        40.772606         -73.968719   \n",
       "\n",
       "    dropoff_latitude  passenger_count       dist  dayofweek  hour  month  year  \n",
       "0          40.760658                1  10.900634          4    18     10  2009  \n",
       "2          40.755962                1   0.607685          5     1      2  2009  \n",
       "3          40.759567                1   1.951639          0    20     10  2011  \n",
       "8          40.732300                1   0.747204          4    11      8  2010  \n",
       "10         40.751492                1   2.606266          3     8      7  2012  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = P1.iloc[:,1:].values\n",
    "y = P1.iloc[:,0].values\n",
    "\n",
    "seed = 101\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          62.5739           5.5182            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         2          57.5837           4.9741           24.04s\n",
      "         3          52.9893           4.5235           17.87s\n",
      "         4          49.7394           4.0513           12.09s\n",
      "         5          45.2714           3.6986            6.07s\n",
      "         6          42.3671           3.3209            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         7          39.0262           3.0021           24.45s\n",
      "         8          36.1776           2.7095           18.18s\n",
      "         9          34.4151           2.4394           12.10s\n",
      "        10          31.6392           2.2232            6.07s\n",
      "        11          29.8741           1.9963            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        12          27.8399           1.7989           24.81s\n",
      "        13          26.0208           1.6437           18.00s\n",
      "        14          25.1391           1.4666           12.07s\n",
      "        15          23.3972           1.3400            6.02s\n",
      "        16          22.2905           1.2069            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        17          21.0597           1.0906           24.35s\n",
      "        18          19.8530           0.9877           18.03s\n",
      "        19          19.5089           0.8897           12.10s\n",
      "        20          18.3987           0.8147            6.06s\n",
      "        21          17.6813           0.7286            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        22          16.9434           0.6625           24.03s\n",
      "        23          16.0980           0.5980           17.90s\n",
      "        24          16.0640           0.5380           12.02s\n",
      "        25          15.3488           0.4824            6.01s\n",
      "        26          14.8534           0.4453            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        27          14.4228           0.4012           24.03s\n",
      "        28          13.7826           0.3646           17.98s\n",
      "        29          13.9432           0.3248           11.88s\n",
      "        30          13.4706           0.2971            5.98s\n",
      "        31          13.1030           0.2689            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        32          12.8560           0.2457           24.43s\n",
      "        33          12.3441           0.2211           17.89s\n",
      "        34          12.6196           0.1988           11.80s\n",
      "        35          12.3002           0.1829            5.98s\n",
      "        36          12.0003           0.1719            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        37          11.8714           0.1478           23.24s\n",
      "        38          11.4322           0.1402           17.42s\n",
      "        39          11.7765           0.1208           11.52s\n",
      "        40          11.5523           0.1144            5.81s\n",
      "        41          11.2931           0.1025            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        42          11.2231           0.0973           23.88s\n",
      "        43          10.8343           0.0858           17.70s\n",
      "        44          11.2190           0.0749           11.54s\n",
      "        45          11.0521           0.0726            5.79s\n",
      "        46          10.8132           0.0684            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        47          10.7759           0.0651           22.51s\n",
      "        48          10.4205           0.0630           17.31s\n",
      "        49          10.8231           0.0457           11.50s\n",
      "        50          10.6811           0.0491            5.75s\n",
      "        51          10.4639           0.0399            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        52          10.4561           0.0484           21.78s\n",
      "        53          10.1384           0.0414           16.50s\n",
      "        54          10.5479           0.0250           10.88s\n",
      "        55          10.4320           0.0336            5.46s\n",
      "        56          10.2206           0.0354            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        57          10.2294           0.0302           20.68s\n",
      "        58           9.9260           0.0253           16.51s\n",
      "        59          10.3418           0.0199           10.90s\n",
      "        60          10.2321           0.0340            5.38s\n",
      "        61          10.0258           0.0271            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        62          10.0562           0.0232           20.46s\n",
      "        63           9.7674           0.0175           15.73s\n",
      "        64          10.1791           0.0109           10.60s\n",
      "        65          10.0817           0.0267            5.31s\n",
      "        66           9.8746           0.0204            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        67           9.9202           0.0173           18.89s\n",
      "        68           9.6390           0.0139           15.86s\n",
      "        69          10.0442           0.0088           10.62s\n",
      "        70           9.9613           0.0166            5.25s\n",
      "        71           9.7635           0.0115            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "        72           9.8150           0.0159           19.57s\n",
      "        73           9.5336           0.0174           14.98s\n",
      "        74           9.9437           0.0014           10.01s\n",
      "        75           9.8694           0.0048            4.95s\n",
      "        76           9.6720           0.0086            0.00s\n",
      "RMSE before gridsearch: 3.299\n",
      "CPU times: user 8min 31s, sys: 4.3 s, total: 8min 36s\n",
      "Wall time: 7min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# To speed up GBM we can utilize warm_start building tree\n",
    "# models incrementally with a handy for loop.\n",
    "modelP1 = GradientBoostingRegressor(random_state=seed, max_depth = 7,\n",
    "                                    learning_rate=0.05,\n",
    "                                    subsample=0.5, warm_start=True, verbose=1)\n",
    "\n",
    "for n_estimators in range(1, 80, 5):\n",
    "    modelP1.set_params(n_estimators=n_estimators)\n",
    "    modelP1.fit(X_train, y_train)\n",
    "\n",
    "y_pred=modelP1.predict(X_test)\n",
    "print('RMSE before gridsearch: %.3f' % np.sqrt(mean_squared_error(y_pred,y_test)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE before gridsearch: 3.402\n",
      "CPU times: user 1min 28s, sys: 4.28 s, total: 1min 32s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "modelP1 = xgb.XGBRegressor(gamma=0, objective=\"reg:linear\",\n",
    "                           nthread=-1)\n",
    "modelP1.fit(X_train,y_train)\n",
    "y_pred=modelP1.predict(X_test)\n",
    "print('RMSE before gridsearch: %.3f' % np.sqrt(mean_squared_error(y_pred,y_test)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed: 24.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100}\n",
      "RMSE after gridsearch: 3.503\n",
      "CPU times: user 1h 13min 46s, sys: 6min 18s, total: 1h 20min 4s\n",
      "Wall time: 24min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "params = {'max_depth':[3,4],\n",
    "          'n_estimators':[100],\n",
    "          'min_child_weight':[1,2],\n",
    "          'learning_rate':[0.05,.1],\n",
    "          'colsample_bytree':[0.5,1],\n",
    "          'gamma':[0,1]}\n",
    "\n",
    "cvx = xgb.XGBRegressor(objective= \"reg:linear\",\n",
    "                           nthread=-1)\n",
    "\n",
    "modelP1 = GridSearchCV(cvx, param_grid=params,\n",
    "                       scoring=make_scorer(mean_squared_error),\n",
    "                       verbose=True)\n",
    "\n",
    "modelP1.fit(X_train,y_train)\n",
    "print(modelP1.best_params_)\n",
    "y_pred = modelP1.predict(X_test)\n",
    "print('RMSE after gridsearch: %.3f' % np.sqrt(mean_squared_error(y_pred,y_test)).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "?XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_dist = {\"max_features\": [4,5,6],\n",
    "              \"min_samples_split\": [4,6,8],\n",
    "              \"min_samples_leaf\": [1,2,3],\n",
    "              \"bootstrap\": [True, False]}\n",
    "\n",
    "rsearch = RandomizedSearchCV(modelP1, param_distributions=param_dist, n_jobs=-1, n_iter=20)\n",
    "rsearch.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelP1=rsearch.best_estimator_\n",
    "print(modelP1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelP1 = ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=20,\n",
    "#          max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "#          min_impurity_split=None, min_samples_leaf=3, min_samples_split=4,\n",
    "#          min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1,\n",
    "#          oob_score=False, random_state=101, verbose=0, warm_start=True)\n",
    "\n",
    "modelP1 = ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=20,\n",
    "          max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "          min_impurity_split=None, min_samples_leaf=1, min_samples_split=6,\n",
    "          min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=-1,\n",
    "          oob_score=False, random_state=101, verbose=0, warm_start=True)\n",
    "modelP1.fit(X_train,y_train)\n",
    "\n",
    "# Best model applyied on the test set\n",
    "y_pred=modelP1.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_pred,y_test)).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = P2.iloc[:,1:13].values\n",
    "y = P2.iloc[:,0].values\n",
    "\n",
    "seed = 101\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "modelP2 = ExtraTreesRegressor(random_state=seed, n_estimators=15,\n",
    "                              max_depth = 15, n_jobs = -1, warm_start=True)\n",
    "\n",
    "modelP2.fit(X_train,y_train)\n",
    "\n",
    "scores = cross_val_score(modelP2, X_train, y_train, cv=3, scoring=make_scorer(mean_squared_error))\n",
    "print('RMSE CV: %.3f +/- %.3f' % (np.sqrt(np.mean(scores)), np.sqrt(np.std(scores))))\n",
    "\n",
    "y_pred=modelP2.predict(X_test)\n",
    "print('RMSE: %.3f' % np.sqrt(mean_squared_error(y_pred,y_test)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_dist = {\"max_features\": [4,5,6],\n",
    "              \"min_samples_split\": [4,6,8],\n",
    "              \"min_samples_leaf\": [1,2,3],\n",
    "              \"bootstrap\": [True, False]}\n",
    "\n",
    "rsearch = RandomizedSearchCV(modelP2, param_distributions=param_dist, n_jobs=-1, n_iter=20)\n",
    "rsearch.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelP2=rsearch.best_estimator_\n",
    "print(modelP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelP2 = ExtraTreesRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
    "#          max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "#          min_impurity_split=None, min_samples_leaf=1, min_samples_split=6,\n",
    "#          min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1,\n",
    "#          oob_score=False, random_state=101, verbose=0, warm_start=True)\n",
    "\n",
    "modelP2 = ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
    "          max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "          min_impurity_split=None, min_samples_leaf=2, min_samples_split=6,\n",
    "          min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=-1,\n",
    "          oob_score=False, random_state=101, verbose=0, warm_start=True)\n",
    "\n",
    "modelP2.fit(X_train,y_train)\n",
    "\n",
    "# Best model applyied on the test set\n",
    "y_pred=modelP2.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_pred,y_test)).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "train_df = read_feather('tmp/train2.feather')\n",
    "\n",
    "# Remove missing values\n",
    "train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "\n",
    "# Remove absurd passenger_count\n",
    "train_df = train_df[(train_df['passenger_count'] >= 1) &\n",
    "                    (train_df['passenger_count'] <= 6)]\n",
    "\n",
    "# Remove negative and extreme fare_amount values\n",
    "train_df = train_df[(train_df['fare_amount'] >= 2.5) & (train_df['fare_amount'] <= 500)]\n",
    "\n",
    "# Remove no displacements\n",
    "train_df = train_df[(train_df['pickup_latitude'] != train_df['dropoff_latitude']) &\n",
    "                    (train_df['pickup_longitude'] != train_df['dropoff_longitude'])]\n",
    "\n",
    "# Remove absurd displacements\n",
    "train_df = train_df[(train_df['pickup_longitude'] > lng2min) & (train_df['pickup_longitude'] < lng1max)]\n",
    "train_df = train_df[(train_df['dropoff_longitude'] > lng2min) & (train_df['dropoff_longitude'] < lng1max)]\n",
    "train_df = train_df[(train_df['pickup_latitude'] > lat2min) & (train_df['pickup_latitude'] < lat1max)]\n",
    "train_df = train_df[(train_df['dropoff_latitude'] > lat2min) & (train_df['dropoff_latitude'] < lat1max)]\n",
    "\n",
    "# Create new features - distance\n",
    "train_df['dist'] = distance(train_df['pickup_latitude'], train_df['pickup_longitude'],\n",
    "                            train_df['dropoff_latitude'], train_df['dropoff_longitude'])\n",
    "\n",
    "# Create new features - dayofweek,hour,month,year\n",
    "train_df['dayofweek'] = train_df['pickup_datetime'].dt.dayofweek.astype('uint8')\n",
    "train_df['hour'] = train_df['pickup_datetime'].dt.hour.astype('uint8')\n",
    "train_df['month'] = train_df['pickup_datetime'].dt.month.astype('uint8')\n",
    "train_df['year'] = train_df['pickup_datetime'].dt.year.astype('uint16')\n",
    "\n",
    "# Create dataframes for the two periods\n",
    "P1 = train_df[(train_df['pickup_datetime'] < '2012-09-01')]\n",
    "P1 = P1.drop(['pickup_datetime'], axis=1)\n",
    "print(P1.shape)\n",
    "P2 = train_df.loc[(train_df['pickup_datetime'] >= '2012-09-01')]\n",
    "P2 = P2.drop(['pickup_datetime'], axis=1)\n",
    "print(P2.shape)\n",
    "\n",
    "# Save memory\n",
    "print(train_df.shape)\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = P1.iloc[:,1:11].values\n",
    "y = P1.iloc[:,0].values\n",
    "\n",
    "for n_estimators in range(22, 32, 2):\n",
    "    modelP1.set_params(n_estimators=n_estimators)\n",
    "    modelP1.fit(X_train, y_train)\n",
    "    \n",
    "y_pred=modelP1.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_pred,y_test)).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = P2.iloc[:,1:11].values\n",
    "y = P2.iloc[:,0].values\n",
    "\n",
    "modelP2.set_params(n_estimators=20, random_state=101, n_jobs = -1, warm_start=True)\n",
    "modelP2.fit(X, y)\n",
    "y_pred=modelP2.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_pred,y_test)).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sample 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "train_df = read_feather('tmp/train3.feather')\n",
    "\n",
    "# Remove missing values\n",
    "train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "\n",
    "# Remove absurd passenger_count\n",
    "train_df = train_df[(train_df['passenger_count'] >= 1) &\n",
    "                    (train_df['passenger_count'] <= 6)]\n",
    "\n",
    "# Remove negative and extreme fare_amount values\n",
    "train_df = train_df[(train_df['fare_amount'] >= 2.5) & (train_df['fare_amount'] <= 500)]\n",
    "\n",
    "# Remove no displacements\n",
    "train_df = train_df[(train_df['pickup_latitude'] != train_df['dropoff_latitude']) &\n",
    "                    (train_df['pickup_longitude'] != train_df['dropoff_longitude'])]\n",
    "\n",
    "# Remove absurd displacements\n",
    "train_df = train_df[(train_df['pickup_longitude'] > lng2min) & (train_df['pickup_longitude'] < lng1max)]\n",
    "train_df = train_df[(train_df['dropoff_longitude'] > lng2min) & (train_df['dropoff_longitude'] < lng1max)]\n",
    "train_df = train_df[(train_df['pickup_latitude'] > lat2min) & (train_df['pickup_latitude'] < lat1max)]\n",
    "train_df = train_df[(train_df['dropoff_latitude'] > lat2min) & (train_df['dropoff_latitude'] < lat1max)]\n",
    "\n",
    "# Create new features - distance\n",
    "train_df['dist'] = distance(train_df['pickup_latitude'], train_df['pickup_longitude'],\n",
    "                            train_df['dropoff_latitude'], train_df['dropoff_longitude'])\n",
    "\n",
    "# Create new features - dayofweek,hour,month,year\n",
    "train_df['dayofweek'] = train_df['pickup_datetime'].dt.dayofweek.astype('uint8')\n",
    "train_df['hour'] = train_df['pickup_datetime'].dt.hour.astype('uint8')\n",
    "train_df['month'] = train_df['pickup_datetime'].dt.month.astype('uint8')\n",
    "train_df['year'] = train_df['pickup_datetime'].dt.year.astype('uint16')\n",
    "\n",
    "# Create dataframes for the two periods\n",
    "P1 = train_df[(train_df['pickup_datetime'] < '2012-09-01')]\n",
    "P1 = P1.drop(['pickup_datetime'], axis=1)\n",
    "print(P1.shape)\n",
    "P2 = train_df.loc[(train_df['pickup_datetime'] >= '2012-09-01')]\n",
    "P2 = P2.drop(['pickup_datetime'], axis=1)\n",
    "print(P2.shape)\n",
    "\n",
    "# Save memory\n",
    "print(train_df.shape)\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = P1.iloc[:,1:11].values\n",
    "y = P1.iloc[:,0].values\n",
    "\n",
    "for n_estimators in range(34, 44, 2):\n",
    "    modelP1.set_params(n_estimators=n_estimators)\n",
    "    modelP1.fit(X_train, y_train)\n",
    "    \n",
    "y_pred=modelP1.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_pred,y_test)).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = P2.iloc[:,1:11].values\n",
    "y = P2.iloc[:,0].values\n",
    "\n",
    "modelP2.set_params(n_estimators=25, random_state=101, n_jobs = -1, warm_start=True)\n",
    "modelP2.fit(X, y)\n",
    "y_pred=modelP2.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_pred,y_test)).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No missing values\n",
    "test_df = read_csv('test.csv', parse_dates=[\"pickup_datetime\"],\n",
    "                  infer_datetime_format=True)\n",
    "\n",
    "test_df['dist'] = distance(test_df['pickup_latitude'], test_df['pickup_longitude'],\n",
    "                           test_df['dropoff_latitude'], test_df['dropoff_longitude'])\n",
    "\n",
    "test_df['dayofweek'] = test_df['pickup_datetime'].dt.dayofweek #.astype('uint8')\n",
    "test_df['hour'] = test_df['pickup_datetime'].dt.hour #.astype('uint8')\n",
    "test_df['month'] = test_df['pickup_datetime'].dt.month #.astype('uint8')\n",
    "test_df['year'] = test_df['pickup_datetime'].dt.year #.astype('uint8')\n",
    "#test_df = test_df.drop(['passenger_count'], axis=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testF = test_df.iloc[:,[2,3,4,5,6,7,8,9,10,11]].values\n",
    "y_predFP1 = modelP1.predict(X_testF).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_testF = test_df.iloc[:,[2,3,4,5,6,7,8,9,10,11]].values\n",
    "#y_predFP1 = modelP1.predict(X_testF).round(3)\n",
    "y_predFP2 = modelP2.predict(X_testF).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(y_predFP1, y_predFP2)\n",
    "plt.xlabel('y_pred P1')\n",
    "plt.ylabel('y_pred P2')\n",
    "plt.xlim(0,150)\n",
    "plt.ylim(0,150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_predFP2 = np.where(y_predFP2 > 150, 100, y_predFP2)\n",
    "#y_predFP2 = np.where((y_predFP2 > 90) & (y_predFP1 < 40), 35, y_predFP2)\n",
    "#y_predFP2 = np.where((y_predFP2 > 40) & (y_predFP1 < 24), 11, y_predFP2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#plt.scatter(y_predFP1, y_predFP2)\n",
    "#plt.xlabel('y_pred P1')\n",
    "#plt.ylabel('y_pred P2')\n",
    "#plt.xlim(0,250)\n",
    "#plt.ylim(0,250)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = DataFrame({'key': test_df.key, 'fare_amountP1': y_predFP1,\n",
    "                        'fare_amountP2': y_predFP2},\n",
    "                       columns = ['key', 'fare_amountP1', 'fare_amountP2'])\n",
    "\n",
    "submission['fare_amount'] = np.where(test_df['pickup_datetime'] < '2012-09-01',\n",
    "                                     submission['fare_amountP1'],\n",
    "                                     submission['fare_amountP2'])\n",
    "\n",
    "submission = submission.drop(['fare_amountP1','fare_amountP2'], axis=1)\n",
    "submission.to_csv('submission.csv', index = False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testP1 = test_df[(test_df['pickup_datetime'] < '2012-09-01')]\n",
    "print(testP1.shape)\n",
    "testP2 = test_df[(test_df['pickup_datetime'] >= '2012-09-01')]\n",
    "print(testP2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
